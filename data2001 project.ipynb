{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f72335-74bd-461a-94d7-175efb2136fa",
   "metadata": {},
   "source": [
    "DATA2001 Project - Greater Sydney Analysis\n",
    "\n",
    "Authors: Lakshya Sakhuja (540863213), Ayush Arora (540906543) and Drishti Nehra (530685294)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83bf24-2ee9-4d90-aa81-edd2f19d91cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the required libraries.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import sql\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sqlalchemy import create_engine\n",
    "from geoalchemy2 import Geometry\n",
    "from urllib.parse import quote_plus\n",
    "from sqlalchemy.engine import URL\n",
    "from IPython.core.magic import register_line_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342194c-391b-4c8b-ade2-cc5cbb71abd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing dependencies (skip this step if already installed)\n",
    "\n",
    "!pip install geopandas\n",
    "!pip install sqlalchemy\n",
    "!pip install ipython-sql sqlalchemy psycopg2-binary\n",
    "!pip install ipython-sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ef24a4-0a23-4fde-8d78-14749964ff00",
   "metadata": {},
   "source": [
    "We are considering that you have a folder named \"data2001\", in which you have the following files:\n",
    "\n",
    "1. This jupyter notebook\n",
    "2. Businesses.csv\n",
    "3. Income.csv\n",
    "4. Population.csv\n",
    "5. Stops.txt\n",
    "6. SA2_2021_AUST_SHP_GDA2020.zip (downloaded exactly from the link given in assignment specifications to the ABS Website)\n",
    "7. db.json (to connect to postgres server)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f183dd-3ae6-4fbd-bff6-0d42804d025a",
   "metadata": {},
   "source": [
    "db.json contains this (in a .json file):\n",
    "\n",
    "{\n",
    "  \"user\": \"postgres\",\n",
    "  \"password\": \"Lakshya@0710\",\n",
    "  \"host\": \"localhost\",\n",
    "  \"port\": 5432,\n",
    "  \"database\": \"data2001_project\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb45a0-adda-44d1-8bde-1b1801016f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the whole notebook universal. \n",
    "\n",
    "target_folder = \"data2001\" # change this name to your folder name.\n",
    "current_path = os.getcwd()\n",
    "\n",
    "while target_folder not in os.listdir(current_path) and current_path != \"/\":\n",
    "    current_path = os.path.dirname(current_path)\n",
    "\n",
    "project_root = os.path.join(current_path, target_folder)\n",
    "os.chdir(project_root)\n",
    "\n",
    "os.environ[\"PROJECT_ROOT\"] = project_root\n",
    "print(\"Project root set to:\", project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c972d6d3-88ab-4ce8-b033-1cdec505a7e7",
   "metadata": {},
   "source": [
    "Expected output (this one will change according to the system):\n",
    "\n",
    "Project root set to: /Users/lakshyasakhuja/data2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c776685d-438e-4621-81b3-f83343a5aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python helper function to read the JSON and connect.\n",
    "\n",
    "def connect_from_json(json_path=\"db.json\"):\n",
    "    project_root = os.environ.get(\"PROJECT_ROOT\", os.getcwd())\n",
    "    full_path = os.path.join(project_root, json_path)\n",
    "\n",
    "    with open(full_path, \"r\") as file:\n",
    "        creds = json.load(file)\n",
    "\n",
    "    password = quote_plus(creds[\"password\"])  # Encode special characters like @ or :\n",
    "    \n",
    "    url = f\"postgresql://{creds['user']}:{password}@{creds['host']}:{creds['port']}/{creds['database']}\"\n",
    "    print(\"Connection URL:\", url)\n",
    "    \n",
    "    return create_engine(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4e7eb-b4fa-48fd-a873-1ec7813141d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the engine to connect.\n",
    "\n",
    "engine = connect_from_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd76d80d-3eda-4719-a5da-fe12f291dc70",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "Connection URL: postgresql://postgres:Lakshya%400710@localhost:5432/data2001_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb98b3-fd89-40a6-acb2-6a9f5b933844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting Stops.txt to stops.csv for better processing.\n",
    "\n",
    "stops = pd.read_csv(\"Stops.txt\", delimiter=\",\")\n",
    "stops.to_csv(\"stops.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132556b8-a2b6-4333-a705-3556a858c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all csv files.\n",
    "\n",
    "businesses = pd.read_csv(\"Businesses.csv\")\n",
    "population = pd.read_csv(\"Population.csv\")\n",
    "income     = pd.read_csv(\"Income.csv\")\n",
    "stops      = pd.read_csv(\"Stops.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b40cd3-70cc-4c4b-b065-824a0993869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzipping the catchments file.\n",
    "\n",
    "zip_path = \"Catchments.zip\" \n",
    "extract_to = \"catchments_files\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "os.chdir(extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d7001-2462-4516-9cd3-bd84d85d2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the working directory.\n",
    "\n",
    "os.chdir(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502c337-145f-414d-9a1d-7314f6dfaab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the businesses dataset (using pre-loaded 'businesses')\n",
    "\n",
    "businesses_df = businesses.copy()\n",
    "businesses_df.columns = [col.lower().replace('-', '').replace(' ', '') for col in businesses_df.columns]\n",
    "\n",
    "for col in businesses_df.columns:\n",
    "    if 'businesses' in col:\n",
    "        businesses_df[col] = pd.to_numeric(businesses_df[col], errors='coerce')\n",
    "\n",
    "businesses_df = businesses_df.dropna(subset=['sa2_code', 'industry_code'])\n",
    "businesses_df['sa2_name'] = businesses_df['sa2_name'].str.strip().str.title()\n",
    "businesses_df['industry_name'] = businesses_df['industry_name'].str.strip().str.title()\n",
    "businesses_df.drop_duplicates(inplace=True)\n",
    "business_columns = [col for col in businesses_df.columns if '_businesses' in col and col != 'total_businesses']\n",
    "businesses_df['total_businesses'] = businesses_df[business_columns].sum(axis=1)\n",
    "businesses_df['has_businesses'] = businesses_df['total_businesses'] > 0\n",
    "\n",
    "businesses_df.to_csv('cleaned_businesses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b48e888-5c91-454d-a816-4733d8d12679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the income dataset (using pre-loaded 'income')\n",
    "\n",
    "income_cleaned = income.copy()\n",
    "income_cleaned.columns = [col.lower().replace('-', '').replace(' ', '') for col in income_cleaned.columns]\n",
    "income_cleaned.rename(columns={'sa2_code21': 'sa2_code'}, inplace=True)\n",
    "\n",
    "for col in ['earners', 'median_age', 'median_income', 'mean_income']:\n",
    "    income_cleaned[col] = pd.to_numeric(income_cleaned[col], errors='coerce')\n",
    "\n",
    "income_cleaned['sa2_name'] = income_cleaned['sa2_name'].str.strip().str.title()\n",
    "income_cleaned.drop_duplicates(inplace=True)\n",
    "income_cleaned['has_income'] = income_cleaned['median_income'] > 0\n",
    "\n",
    "income_cleaned.to_csv('cleaned_income.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c97d1-1203-4ff7-a936-76de733b096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the population dataset (using pre-loaded 'population')\n",
    "\n",
    "population_cleaned = population.copy()\n",
    "population_cleaned.columns = [col.lower().replace('-', '').replace(' ', '') for col in population_cleaned.columns]\n",
    "\n",
    "for col in population_cleaned.columns:\n",
    "    if '_people' in col or col == 'total_people':\n",
    "        population_cleaned[col] = pd.to_numeric(population_cleaned[col], errors='coerce')\n",
    "\n",
    "population_cleaned['sa2_name'] = population_cleaned['sa2_name'].str.strip().str.title()\n",
    "population_cleaned.drop_duplicates(inplace=True)\n",
    "population_cleaned['has_population'] = population_cleaned['total_people'] > 0\n",
    "\n",
    "population_cleaned.to_csv('cleaned_population.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfea6ec-d85d-493a-8199-09572faf4d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the stops dataset.\n",
    "\n",
    "stops_cleaned = stops.copy()\n",
    "stops_cleaned['stop_name'] = stops_cleaned['stop_name'].str.strip().str.title()\n",
    "stops_cleaned.drop(columns=['stop_code'], inplace=True)\n",
    "stops_cleaned['location_type'] = stops_cleaned['location_type'].fillna(0)\n",
    "stops_cleaned['location_type'] = stops_cleaned['location_type'].astype(int)\n",
    "stops_cleaned.drop(columns=['has_coordinates'], errors='ignore', inplace=True)\n",
    "stops_cleaned.drop_duplicates(inplace=True)\n",
    "\n",
    "stops_cleaned.to_csv('cleaned_stops.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558869f4-0d66-4697-9dcb-bf32b6793629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the working directory.\n",
    "\n",
    "os.chdir(os.path.join(project_root, \"catchments_files/catchments\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6765803b-5051-45a9-8e2a-ab7b208e971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning catchments_primary shape file and uploading it.\n",
    "\n",
    "primary = gpd.read_file(\"catchments_primary.shp\").to_crs(epsg=4326)\n",
    "primary[\"USE_ID\"] = pd.to_numeric(primary[\"USE_ID\"], errors=\"coerce\").astype(\"Int64\")\n",
    "primary[\"ADD_DATE\"] = pd.to_datetime(primary[\"ADD_DATE\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "year_columns = [\n",
    "    \"KINDERGART\", \"YEAR1\", \"YEAR2\", \"YEAR3\", \"YEAR4\", \"YEAR5\", \n",
    "    \"YEAR6\", \"YEAR7\", \"YEAR8\", \"YEAR9\", \"YEAR10\", \"YEAR11\", \"YEAR12\"\n",
    "]\n",
    "\n",
    "for col in year_columns:\n",
    "    primary[col] = primary[col].map({\"Y\": True, \"N\": False}).astype(\"boolean\")\n",
    "\n",
    "engine = connect_from_json()\n",
    "primary.to_postgis(\"school_catchments_primary\", engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef75e4c-e07c-4f2c-b89c-3c89bde182b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning catchments_secondary shape file and uploading it.\n",
    "\n",
    "secondary = gpd.read_file(\"catchments_secondary.shp\").to_crs(epsg=4326)\n",
    "secondary['USE_ID'] = pd.to_numeric(secondary['USE_ID'], errors='coerce')\n",
    "secondary['ADD_DATE'] = pd.to_datetime(secondary['ADD_DATE'], errors='coerce')\n",
    "yn_fields = ['KINDERGART', 'PREP', 'YEAR1', 'YEAR2', 'YEAR3', 'YEAR4', 'YEAR5',\n",
    "             'YEAR6', 'YEAR7', 'YEAR8', 'YEAR9', 'YEAR10', 'YEAR11', 'YEAR12']\n",
    "\n",
    "for col in yn_fields:\n",
    "    if col in secondary.columns:\n",
    "        secondary[col] = secondary[col].map({'Y': True, 'N': False})\n",
    "\n",
    "secondary['CATCH_TYPE'] = secondary['CATCH_TYPE'].astype('category')\n",
    "secondary['USE_DESC'] = secondary['USE_DESC'].astype('category')\n",
    "\n",
    "secondary.to_postgis(\"school_catchments_secondary\", engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cdcc6-6df5-4e2c-b7d0-9726cc529830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning catchments_future shape file and uploading it.\n",
    "\n",
    "future = gpd.read_file(\"catchments_future.shp\").to_crs(epsg=4326)\n",
    "future['USE_ID'] = pd.to_numeric(future['USE_ID'], errors='coerce')\n",
    "future['ADD_DATE'] = pd.to_datetime(future['ADD_DATE'], errors='coerce')\n",
    "\n",
    "year_fields = ['KINDERGART', 'YEAR1', 'YEAR2', 'YEAR3', 'YEAR4', 'YEAR5',\n",
    "               'YEAR6', 'YEAR7', 'YEAR8', 'YEAR9', 'YEAR10', 'YEAR11', 'YEAR12']\n",
    "\n",
    "for col in year_fields:\n",
    "    if col in future.columns:\n",
    "        future[col] = future[col].apply(lambda x: True if x == 2024 else False)\n",
    "\n",
    "future['CATCH_TYPE'] = future['CATCH_TYPE'].astype('category')\n",
    "future['USE_DESC'] = future['USE_DESC'].astype('category')\n",
    "\n",
    "future.to_postgis(\"school_catchments_future\", engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6520f656-6c62-4c4e-b394-c2a5f511fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the working directory.\n",
    "\n",
    "os.chdir(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29032225-05a4-4fbc-870b-e5baab0c9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading the remaining three datasets.\n",
    "\n",
    "income_df = pd.read_csv('cleaned_income.csv')\n",
    "income_df.to_sql(\"income_data\", engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "population_df = pd.read_csv(\"cleaned_population.csv\")\n",
    "population_df.to_sql(\"population_data\", engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "business_df = pd.read_csv(\"cleaned_businesses.csv\")\n",
    "business_df.to_sql(\"business_data\", engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "stops_df = pd.read_csv(\"cleaned_stops.csv\")\n",
    "stops_df.to_sql(\"stops_data\", engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4bb006-6324-47eb-a3c3-61a2238d96f8",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "718"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c71bd3-4ffe-4493-8674-1bd4eca3085a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# working on the shapefile of the SA2 digital boundaries and uploading it\n",
    "\n",
    "zip_path = \"SA2_2021_AUST_SHP_GDA2020.zip\"\n",
    "extract_dir = \"sa2_unzipped\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "sa2_path = os.path.join(extract_dir, \"SA2_2021_AUST_GDA2020.shp\")\n",
    "sa2 = gpd.read_file(sa2_path).to_crs(epsg=4326)\n",
    "\n",
    "engine = connect_from_json()\n",
    "sa2.to_postgis(\"sa2_regions\", engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b721db3-af43-4ca7-abba-5aaafa64d1d5",
   "metadata": {},
   "source": [
    "Expcted output:\n",
    "\n",
    "Connection URL: postgresql://postgres:Lakshya%400710@localhost:5432/data2001_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf8da71-ea75-4de1-925c-accea28883a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out only the 'Greater Sydney' regions from SA2 table.\n",
    "\n",
    "sa2_gs = gpd.read_postgis(\n",
    "    \"\"\"\n",
    "    SELECT * FROM sa2_regions\n",
    "    WHERE \"GCC_NAME21\" = 'Greater Sydney'\n",
    "    \"\"\",\n",
    "    engine,\n",
    "    geom_col=\"geometry\"\n",
    ")\n",
    "\n",
    "sa2_gs.to_postgis(\"sa2_greater_sydney\", engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "sa2[\"bbox\"] = sa2.geometry.bounds.apply(lambda b: (b.minx, b.miny, b.maxx, b.maxy), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9531962-4a8e-4065-b805-3a34ece8c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that returns all POIs from the API within a specified bounding box of coordinates.\n",
    "\n",
    "def fetch_pois(minx, miny, maxx, maxy):\n",
    "    url = \"https://maps.six.nsw.gov.au/arcgis/rest/services/public/NSW_POI/MapServer/0/query\"\n",
    "    params = {\n",
    "        \"f\": \"geojson\",\n",
    "        \"geometry\": f\"{minx},{miny},{maxx},{maxy}\",\n",
    "        \"geometryType\": \"esriGeometryEnvelope\",\n",
    "        \"inSR\": 4326,\n",
    "        \"spatialRel\": \"esriSpatialRelIntersects\",\n",
    "        \"outFields\": \"*\",\n",
    "        \"returnGeometry\": \"true\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"API Error:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "output_path = \"pois_partial.csv\"\n",
    "if os.path.exists(output_path):\n",
    "    pois_df = pd.read_csv(output_path)\n",
    "    processed_sa2s = set(pois_df['sa2_name'])\n",
    "else:\n",
    "    pois_df = pd.DataFrame()\n",
    "    processed_sa2s = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b55af-6905-4dc9-b59b-d7fa2d1e1f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes and stores bounding boxes for each SA2 region as (minx, miny, maxx, maxy) tuples.\n",
    "\n",
    "sa2[\"bbox\"] = sa2.geometry.bounds.apply(lambda row: (row.minx, row.miny, row.maxx, row.maxy), axis=1)\n",
    "sa2[\"bbox\"] = sa2.geometry.bounds.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d905a1a3-7d59-4d4d-898e-9b32de638014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all column names to help choose primary and foreign keys (if necessary).\n",
    "\n",
    "tables = [\n",
    "    \"business_data\",\n",
    "    \"income_data\",\n",
    "    \"population_data\",\n",
    "    \"sa2_greater_sydney\",\n",
    "    \"sa2_regions\",\n",
    "    \"school_catchments_primary\",\n",
    "    \"school_catchments_secondary\",\n",
    "    \"school_catchments_future\",\n",
    "    \"stops_data\",\n",
    "]\n",
    "\n",
    "def get_columns(table, is_geo=False):\n",
    "    try:\n",
    "        if is_geo:\n",
    "            df = gpd.read_postgis(f'SELECT * FROM {table} LIMIT 1', engine, geom_col=\"geometry\")\n",
    "        else:\n",
    "            df = pd.read_sql(f'SELECT * FROM {table} LIMIT 1', engine)\n",
    "        print(f\"\\nTable: {table}\")\n",
    "        print(list(df.columns))\n",
    "    except Exception as e:\n",
    "        print(f\"\\nTable: {table} â€” ERROR: {e}\")\n",
    "\n",
    "for table in tables:\n",
    "    is_geo = any(kw in table for kw in [\"catchments\", \"regions\", \"pois\"])\n",
    "    get_columns(table, is_geo=is_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bee751-fd94-4e0a-92cb-19e9bea51481",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "Table: business_data\n",
    "['industry_code', 'industry_name', 'sa2_code', 'sa2_name', '0_to_50k_businesses', '50k_to_200k_businesses', '200k_to_2m_businesses', '2m_to_5m_businesses', '5m_to_10m_businesses', '10m_or_more_businesses', 'total_businesses', 'has_businesses', 'id']\n",
    "\n",
    "Table: income_data\n",
    "['sa2_code', 'sa2_name', 'earners', 'median_age', 'median_income', 'mean_income', 'has_income']\n",
    "\n",
    "Table: population_data\n",
    "['sa2_code', 'sa2_name', '04_people', '59_people', '1014_people', '1519_people', '2024_people', '2529_people', '3034_people', '3539_people', '4044_people', '4549_people', '5054_people', '5559_people', '6064_people', '6569_people', '7074_people', '7579_people', '8084_people', '85andover_people', 'total_people', 'has_population']\n",
    "\n",
    "Table: sa2_greater_sydney\n",
    "['sa2_code', 'sa2_name', 'chg_flag', 'chg_lbl', 'sa3_code', 'sa3_name', 'sa4_code', 'sa4_name', 'gcc_code', 'gcc_name', 'ste_code', 'ste_name', 'aus_code', 'aus_name', 'areasqkm', 'loci_uri', 'geometry']\n",
    "\n",
    "Table: sa2_regions\n",
    "['sa2_code', 'sa2_name', 'chg_flag', 'chg_lbl', 'sa3_code', 'sa3_name', 'sa4_code', 'sa4_name', 'gcc_code', 'gcc_name', 'ste_code', 'ste_name', 'aus_code', 'aus_name', 'areasqkm', 'loci_uri', 'geometry']\n",
    "\n",
    "Table: school_catchments_primary\n",
    "['use_id', 'catch_type', 'use_desc', 'add_date', 'kindergart', 'year1', 'year2', 'year3', 'year4', 'year5', 'year6', 'year7', 'year8', 'year9', 'year10', 'year11', 'year12', 'priority', 'geometry']\n",
    "\n",
    "Table: school_catchments_secondary\n",
    "['use_id', 'catch_type', 'use_desc', 'add_date', 'kindergart', 'year1', 'year2', 'year3', 'year4', 'year5', 'year6', 'year7', 'year8', 'year9', 'year10', 'year11', 'year12', 'priority', 'geometry']\n",
    "\n",
    "Table: school_catchments_future\n",
    "['use_id', 'CATCH_TYPE', 'USE_DESC', 'ADD_DATE', 'kindergart', 'year1', 'year2', 'year3', 'year4', 'year5', 'year6', 'year7', 'year8', 'year9', 'year10', 'year11', 'year12', 'geometry']\n",
    "\n",
    "Table: stops_data\n",
    "['stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'location_type', 'parent_station', 'wheelchair_boarding', 'platform_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b34e86-f6a1-4c68-9a10-7ff45625d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7284b923-ef75-49c2-b566-9952719b5221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_url(json_path=\"db.json\"):\n",
    "    with open(json_path, \"r\") as file:\n",
    "        creds = json.load(file)\n",
    "    password = quote_plus(creds[\"password\"])\n",
    "    return f\"postgresql://{creds['user']}:{password}@{creds['host']}:{creds['port']}/{creds['database']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3028a437-464d-4732-bdd1-3870a977db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql {get_sql_url()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d836d503-249a-4adc-9fa0-235f859e2671",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- changing column names for consistency.\n",
    "\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"SA2_CODE21\" TO sa2_code;\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"SA2_NAME21\" TO sa2_name;\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"CHG_FLAG21\" TO chg_flag;\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"CHG_LBL21\" TO chg_lbl;\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"SA3_CODE21\" TO sa3_code;\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"SA3_NAME21\" TO sa3_name;\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"SA4_CODE21\" TO sa4_code;\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"SA4_NAME21\" TO sa4_name;\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"GCC_CODE21\" TO gcc_code;\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"GCC_NAME21\" TO gcc_name;\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"STE_CODE21\" TO ste_code;\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"STE_NAME21\" TO ste_name;\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"AUS_CODE21\" TO aus_code;\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"AUS_NAME21\" TO aus_name;\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"AREASQKM21\" TO areasqkm;\n",
    "ALTER TABLE sa2_regions RENAME COLUMN \"LOCI_URI21\" TO loci_uri;\n",
    "\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"SA2_CODE21\" TO sa2_code;\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"SA2_NAME21\" TO sa2_name;\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"CHG_FLAG21\" TO chg_flag;\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"CHG_LBL21\" TO chg_lbl;\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"SA3_CODE21\" TO sa3_code;\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"SA3_NAME21\" TO sa3_name;\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"SA4_CODE21\" TO sa4_code;\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"SA4_NAME21\" TO sa4_name;\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"GCC_CODE21\" TO gcc_code;\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"GCC_NAME21\" TO gcc_name;\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"STE_CODE21\" TO ste_code;\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"STE_NAME21\" TO ste_name;\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"AUS_CODE21\" TO aus_code;\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"AUS_NAME21\" TO aus_name;\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"AREASQKM21\" TO areasqkm;\n",
    "ALTER TABLE sa2_greater_sydney RENAME COLUMN \"LOCI_URI21\" TO loci_uri;\n",
    "\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"USE_ID\" TO use_id;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"CATCH_TYPE\" TO catch_type;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"USE_DESC\" TO use_desc;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"ADD_DATE\" TO add_date;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"PRIORITY\" TO priority;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"KINDERGART\" TO kindergart;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"YEAR1\" TO year1;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"YEAR2\" TO year2;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"YEAR3\" TO year3;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"YEAR4\" TO year4;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"YEAR5\" TO year5;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"YEAR6\" TO year6;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"YEAR7\" TO year7;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"YEAR8\" TO year8;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"YEAR9\" TO year9;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"YEAR10\" TO year10;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"YEAR11\" TO year11;\n",
    "ALTER TABLE school_catchments_primary RENAME COLUMN \"YEAR12\" TO year12;\n",
    "\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"USE_ID\" TO use_id;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"CATCH_TYPE\" TO catch_type;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"USE_DESC\" TO use_desc;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"ADD_DATE\" TO add_date;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"PRIORITY\" TO priority;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"KINDERGART\" TO kindergart;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"YEAR1\" TO year1;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"YEAR2\" TO year2;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"YEAR3\" TO year3;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"YEAR4\" TO year4;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"YEAR5\" TO year5;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"YEAR6\" TO year6;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"YEAR7\" TO year7;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"YEAR8\" TO year8;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"YEAR9\" TO year9;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"YEAR10\" TO year10;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"YEAR11\" TO year11;\n",
    "ALTER TABLE school_catchments_secondary RENAME COLUMN \"YEAR12\" TO year12;\n",
    "\n",
    "ALTER TABLE school_catchments_future RENAME COLUMN \"USE_ID\" TO use_id;\n",
    "ALTER TABLE school_catchments_future RENAME COLUMN \"KINDERGART\" TO kindergart;\n",
    "ALTER TABLE school_catchments_future RENAME COLUMN \"YEAR1\" TO year1;\n",
    "ALTER TABLE school_catchments_future RENAME COLUMN \"YEAR2\" TO year2;\n",
    "ALTER TABLE school_catchments_future RENAME COLUMN \"YEAR3\" TO year3;\n",
    "ALTER TABLE school_catchments_future RENAME COLUMN \"YEAR4\" TO year4;\n",
    "ALTER TABLE school_catchments_future RENAME COLUMN \"YEAR5\" TO year5;\n",
    "ALTER TABLE school_catchments_future RENAME COLUMN \"YEAR6\" TO year6;\n",
    "ALTER TABLE school_catchments_future RENAME COLUMN \"YEAR7\" TO year7;\n",
    "ALTER TABLE school_catchments_future RENAME COLUMN \"YEAR8\" TO year8;\n",
    "ALTER TABLE school_catchments_future RENAME COLUMN \"YEAR9\" TO year9;\n",
    "ALTER TABLE school_catchments_future RENAME COLUMN \"YEAR10\" TO year10;\n",
    "ALTER TABLE school_catchments_future RENAME COLUMN \"YEAR11\" TO year11;\n",
    "ALTER TABLE school_catchments_future RENAME COLUMN \"YEAR12\" TO year12;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e3685-9b81-4269-a44f-3e070939eb65",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "* postgresql://postgres:***@localhost:5432/data2001_project\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "Done.\n",
    "\n",
    "[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7f744-2e88-47b8-a772-f755b6b65d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts Points of Interest (POIs) for all SA2 regions within the selected SA4 areas using the NSW POI API.\n",
    "# avoids reprocessing previously completed regions by checking a partial CSV output.\n",
    "# fetched POIs are saved incrementally to CSV and finally written to a PostGIS table as a GeoDataFrame.\n",
    "\n",
    "engine = connect_from_json()\n",
    "\n",
    "selected_sa4s = [\n",
    "    \"Sydney - Blacktown\",\n",
    "    \"Sydney - City and Inner South\",\n",
    "    \"Sydney - Eastern Suburbs\"\n",
    "]\n",
    "\n",
    "sa2 = gpd.read_postgis(\n",
    "    \"\"\"\n",
    "    SELECT * FROM sa2_regions\n",
    "    WHERE sa4_name IN %(sa4s)s\n",
    "    \"\"\",\n",
    "    engine,\n",
    "    params={\"sa4s\": tuple(selected_sa4s)},\n",
    "    geom_col=\"geometry\"\n",
    ")\n",
    "\n",
    "sa2[\"bbox\"] = sa2.geometry.bounds.apply(lambda row: (row.minx, row.miny, row.maxx, row.maxy), axis=1)\n",
    "\n",
    "output_path = \"selected_sa4_pois_partial.csv\"\n",
    "if os.path.exists(output_path):\n",
    "    pois_df = pd.read_csv(output_path)\n",
    "    processed_sa2s = set(pois_df['sa2_name'])\n",
    "else:\n",
    "    pois_df = pd.DataFrame()\n",
    "    processed_sa2s = set()\n",
    "\n",
    "all_pois = []\n",
    "\n",
    "for idx, row in sa2.iterrows():\n",
    "    sa2_name = row[\"sa2_name\"]\n",
    "    if sa2_name in processed_sa2s:\n",
    "        print(f\"Skipping {sa2_name} (already processed)\")\n",
    "        continue\n",
    "\n",
    "    minx, miny, maxx, maxy = row[\"bbox\"]\n",
    "    print(f\"Processing {idx + 1}/{len(sa2)}: {sa2_name}\")\n",
    "\n",
    "    result = fetch_pois(minx, miny, maxx, maxy)\n",
    "\n",
    "    if result and \"features\" in result:\n",
    "        for poi in result[\"features\"]:\n",
    "            all_pois.append({\n",
    "                \"sa2_name\": sa2_name,\n",
    "                \"poi_name\": poi[\"properties\"].get(\"poiname\") or poi[\"properties\"].get(\"poilabel\") or \"Unnamed\",\n",
    "                \"category\": poi[\"properties\"].get(\"poitype\") or \"Unknown\",\n",
    "                \"longitude\": poi[\"geometry\"][\"coordinates\"][0],\n",
    "                \"latitude\": poi[\"geometry\"][\"coordinates\"][1],\n",
    "            })\n",
    "\n",
    "    if all_pois:\n",
    "        new_df = pd.DataFrame(all_pois, columns=[\"sa2_name\", \"poi_name\", \"category\", \"longitude\", \"latitude\"])\n",
    "        pois_df = pd.concat([pois_df, new_df], ignore_index=True)\n",
    "        pois_df.to_csv(output_path, index=False)\n",
    "        all_pois = []\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "pois_df = pd.read_csv(output_path)\n",
    "pois_gdf = gpd.GeoDataFrame(\n",
    "    pois_df,\n",
    "    geometry=[Point(xy) for xy in zip(pois_df[\"longitude\"], pois_df[\"latitude\"])],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "pois_gdf.to_postgis(\"selected_sa4_pois\", engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a495488-bc44-4663-a43a-70c59c8f0354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "281394ee-d552-49b3-bde1-d9a5acd78f71",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "Connection URL: postgresql://postgres:Lakshya%400710@localhost:5432/data2001_project\n",
    "\n",
    "Processing 1/72: Blacktown (East) - Kings Park\n",
    "\n",
    "Processing 2/72: Blacktown (North) - Marayong\n",
    "\n",
    "Processing 3/72: Doonside - Woodcroft\n",
    "\n",
    "Processing 4/72: Lalor Park - Kings Langley\n",
    "\n",
    "Processing 5/72: Blacktown - South\n",
    "\n",
    "Processing 6/72: Blacktown - West\n",
    "\n",
    "Processing 7/72: Seven Hills - Prospect\n",
    "\n",
    "Processing 8/72: Toongabbie - West\n",
    "\n",
    "Processing 9/72: Glenwood\n",
    "\n",
    "Processing 10/72: Acacia Gardens\n",
    "\n",
    "Processing 11/72: Quakers Hill\n",
    "\n",
    "Processing 12/72: Kellyville Ridge - The Ponds\n",
    "\n",
    "Processing 13/72: Marsden Park - Shanes Park\n",
    "\n",
    "Processing 14/72: Riverstone\n",
    "\n",
    "Processing 15/72: Schofields (West) - Colebee\n",
    "\n",
    "Processing 16/72: Schofields - East\n",
    "\n",
    "Processing 17/72: Stanhope Gardens - Parklea\n",
    "\n",
    "Processing 18/72: Bidwill - Hebersham - Emerton\n",
    "\n",
    "Processing 19/72: Glendenning - Dean Park\n",
    "\n",
    "Processing 20/72: Hassall Grove - Plumpton\n",
    "\n",
    "Processing 21/72: Lethbridge Park - Tregear\n",
    "\n",
    "Processing 22/72: Mount Druitt - Whalan\n",
    "\n",
    "Processing 23/72: Prospect Reservoir\n",
    "\n",
    "Processing 24/72: Rooty Hill - Minchinbury\n",
    "\n",
    "Processing 25/72: Banksmeadow\n",
    "\n",
    "Processing 26/72: Botany\n",
    "\n",
    "Processing 27/72: Pagewood - Hillsdale - Daceyville\n",
    "\n",
    "Processing 28/72: Port Botany Industrial\n",
    "\n",
    "Processing 29/72: Sydney Airport\n",
    "\n",
    "Processing 30/72: Eastlakes\n",
    "\n",
    "Processing 31/72: Mascot\n",
    "\n",
    "Processing 32/72: Petersham - Stanmore\n",
    "\n",
    "Processing 33/72: Sydenham - Tempe - St Peters\n",
    "\n",
    "Processing 34/72: Marrickville - North\n",
    "\n",
    "Processing 35/72: Marrickville - South\n",
    "\n",
    "Processing 36/72: Darlinghurst\n",
    "\n",
    "Processing 37/72: Erskineville - Alexandria\n",
    "\n",
    "Processing 38/72: Glebe - Forest Lodge\n",
    "\n",
    "Processing 39/72: Potts Point - Woolloomooloo\n",
    "\n",
    "Processing 40/72: Surry Hills\n",
    "\n",
    "Processing 41/72: Camperdown - Darlington\n",
    "\n",
    "Processing 42/72: Chippendale\n",
    "\n",
    "Processing 43/72: Newtown (NSW)\n",
    "\n",
    "Processing 44/72: Pyrmont\n",
    "\n",
    "Processing 45/72: Redfern\n",
    "\n",
    "Processing 46/72: Rosebery - Beaconsfield\n",
    "\n",
    "Processing 47/72: Sydney (North) - Millers Point\n",
    "\n",
    "Processing 48/72: Sydney (South) - Haymarket\n",
    "\n",
    "Processing 49/72: Ultimo\n",
    "\n",
    "Processing 50/72: Waterloo\n",
    "\n",
    "Processing 51/72: Zetland\n",
    "\n",
    "Processing 52/72: Bondi - Tamarama - Bronte\n",
    "\n",
    "Processing 53/72: Bondi Beach - North Bondi\n",
    "\n",
    "Processing 54/72: Bondi Junction - Waverly\n",
    "\n",
    "Processing 55/72: Centennial Park\n",
    "\n",
    "Processing 56/72: Dover Heights\n",
    "\n",
    "Processing 57/72: Paddington - Moore Park\n",
    "\n",
    "Processing 58/72: Rose Bay - Vaucluse - Watsons Bay\n",
    "\n",
    "Processing 59/72: Woollahra\n",
    "\n",
    "Processing 60/72: Bellevue Hill\n",
    "\n",
    "Processing 61/72: Double Bay - Darling Point\n",
    "\n",
    "Processing 62/72: Kensington (NSW)\n",
    "\n",
    "Processing 63/72: Kingsford\n",
    "\n",
    "Processing 64/72: Maroubra - North\n",
    "\n",
    "Processing 65/72: Maroubra - South\n",
    "\n",
    "Processing 66/72: Maroubra - West\n",
    "\n",
    "Processing 67/72: Randwick - North\n",
    "\n",
    "Processing 68/72: Randwick - South\n",
    "\n",
    "Processing 69/72: Coogee - Clovelly\n",
    "\n",
    "Processing 70/72: Malabar - La Perouse\n",
    "\n",
    "Processing 71/72: Matraville - Chifley\n",
    "\n",
    "Processing 72/72: South Coogee\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91987136-55e7-4018-9e12-88937e817702",
   "metadata": {},
   "source": [
    "After running the last cell, run the .sql files in the following order:\n",
    "\n",
    "1. Primary Keys and Foreign Keys.sql\n",
    "2. Task3-1.sql (run it step by step from 1 to 8)\n",
    "3. Task3-2.sql (run it step by step from 1 to 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
